# 经济日报：规范人工智能“玩家”是比叫停大模型研究更紧迫的需求

![](https://inews.gtimg.com/om_bt/O10q151goSdNRT8IHlWSFUa2edi7rwx625IFrIaxh-_TUAA/1000)
ChatGPT引发的人工智能大模型竞赛，令很多人担心人工智能失控。3月底国际上就有一批专家发出联名公开信呼吁：立即暂停训练比GPT-4更强大的人工智能系统。人工智能大模型要按暂停键吗？

从可行性来看，这个暂停键目前还没人能按下去。就在5月10日，谷歌宣布推出最新的大型语言模型PaLM
2，称其在部分任务上已经超越GPT-4。这很明显是无视了公开信的呼吁，公开对GPT-4等竞争对手发起挑战。

人工智能大模型已经成为当前国际投资热点，各方不断加码，竞赛不断提速。技术狂飙的背后，是科技企业和风险投资公司的盈利冲动，除非触发政府部门的强制干涉，依赖行业自律按下暂停键的目标不太可能实现。此外，目前大模型的权力集中在“大玩家”手中，现实中行业垄断的危害比未来可能出现的人工智能危害更迫在眉睫，与简单叫停大模型研究相比，规范人工智能“玩家”是更紧迫的需求。技术创新难以抑制，与其叫停研究，还不如尽快制定和完善人工智能治理相关法规，让创新研究在阳光下运行，在规则内运行。

从复杂性来看，大模型的健康发展本身就有赖于技术进步，需要以人工智能来监督、限制人工智能。

人工智能大模型“涌现”出写文章、做翻译、写代码、制图像等许多意想不到的能力，部分能力和人不相上下，生产速度则远超人类，已让许多人感受到冲击。它为什么能够达到目前的能力？目前大模型的“智能涌现”是个黑箱，没有人能理解、预测或可靠地控制这些大模型，甚至模型的创造者也不能。

对人工智能大模型开黑箱、对数据泄露上锁、对虚假内容做标记、对网络攻击等危险行为实施监测防范，一方面需要从科研伦理规范上增加透明度，让大模型研发、训练、生成、推理的链路变得更加透明；另一方面也需要使用好技术工具，用人工智能大模型来帮助人类理解和控制人工智能大模型。这话听起来绕口，实际已有成功案例。ChatGPT的缔造者美国OpenAI公司近日发布一项成果，就是用GPT-4大模型来自动解释GPT-2大模型的行为，在人工智能可解释性研究领域获得重大突破。

眼见可为虚，耳听也不为实。人工智能大模型以其彪悍的信息造假能力，对社会信任带来重大冲击。从一些调查结果看，不少人赞同暂停超越GPT-4能力的大模型研究，先解决已知风险再按启动键。人工智能是国家战略技术，是未来引领经济发展的助推器。对中国人工智能产业而言，需要的不是暂停研究，而是吸取国外大模型发展的前车之鉴，让大模型研发在合规下运行。

科技治理为科技创新保驾护航，人工智能治理已成为重要的国际科技竞争力。大禹治水，“堵”不如“疏”。在研发人工智能大模型的同时，构建高效透明的科研伦理商谈审批机制，拆壁垒、缓痛点、疏堵点，才能建立起更有竞争力、更可持续发展的人工智能产业生态。
（本文来源：经济日报 作者：佘惠敏）

