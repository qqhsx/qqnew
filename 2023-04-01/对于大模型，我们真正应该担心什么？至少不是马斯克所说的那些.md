# 对于大模型，我们真正应该担心什么？至少不是马斯克所说的那些

![](https://inews.gtimg.com/news_bt/OQ7i8XDLqS4L47Ntgx71P10fxSa9lvBtXEqAun6nSqpf8AA/1000)
**划重点**

  * _1_ 对签名者的名单要谨慎对待，因为有报道称某些名字被添加到名单里是恶作剧。
  * _2_ 纽约城市大学新闻学教授杰夫·贾维斯说：“这是道德恐慌的一个典型标本：呼吁暂停训练人工智能。”
  * _3_ 风险来自于大型语言模型在一个压迫性体系中的使用，比假想的人工智能反乌托邦前景要具体和紧迫得多。
  * _4_ 公开信渲染人工智能的末日图景，本身也吊诡地充满了人工智能的炒作，使我们更难解决真实的、正在发生的人工智能危害。
  * _5_ 我们需要担心的是，大型语言模型的权力集中在大玩家手中，复制压迫体系，破坏信息生态系统，以及通过浪费能源资源破坏自然生态系统。

**文 / 胡泳 北京大学新闻与传播学院教授**

在GPT-4火热出炉、人工智能军备竞赛方兴未艾的形势下，包括埃隆·马斯克（Elon Musk）和苹果公司联合创始人史蒂夫·沃兹尼亚克（Steve
Wozniak）在内的千余名科技领袖签署了一封公开信，恳请研究人员在六个月内暂停开发比GPT-4更强大的人工智能系统。

首先，面对人工智能，公开信是老套路了。2015年1月，霍金（Stephen
Hawking）和马斯克及数十位人工智能专家就签署过一封关于人工智能的公开信，呼吁对人工智能的社会影响进行研究，并阻止研究人员创造出不安全或无法控制的系统。2018
年，组织这一次公开信的生命未来研究所（Future of Life
Institute，FLI）还起草过另外一封信，呼吁制定“针对致命自主武器的法律”，签署人当中也有马斯克。

![](https://inews.gtimg.com/news_bt/OOa-NR0_9HlXVjyRrvarlGg2iq-h95tu8Hf6OwGcUICskAA/1000)
其次，对签名者的名单要谨慎对待，因为有报道称某些名字被添加到名单里是恶作剧，例如OpenAI首席执行官山姆·阿尔特曼（Sam
Altman）也曾短暂地现身其中。有些签署者明确表示并不同意公开信的所有内容，只是因为“传达的精神是对的”而表示支持。当然，更多的人工智能研究者和专家公开表示不同意这封信的提议和方法。

信中提到了对错误信息传播、劳动力市场自动化风险以及文明失控的可能性的担忧。以下是要点：

*** 失控的人工智能**

各家公司正在竞相开发先进的人工智能技术，连创造者都无法“理解、预测或可靠地控制”。

信中写道：“我们必须问自己：我们应该让机器用宣传和不真实的东西充斥我们的信息渠道吗？我们是否应该把所有的工作都自动化，包括那些有成就感的工作？我们是否应该开发非人类的大脑，使其最终在数量上超过我们，在智慧上胜过我们，淘汰并取代我们？我们是否应该冒失去对我们文明控制的风险？这些决定绝不能委托给未经选举的技术领袖。只有当我们确信强大的人工智能系统的效果是积极的，其风险是可控的，才应该开发。”

*** “危险的竞赛”**

这封信警告说，人工智能公司陷入了“开发和部署”新的先进系统的“失控竞赛”。 最近几个月，OpenAI
的ChatGPT的病毒式流行似乎促使其他公司加速发布自己的 AI 产品。

公开信敦促企业收获“人工智能之夏”的回报，同时让社会有机会适应新技术，而不是匆忙进入“毫无准备的秋天”。

*** 六个月的暂停期**

信中说，暂停开发将为人工智能系统引入“共享安全协议”提供时间。信中还称，“如果无法快速实施这样的暂停，政府应该介入并制定暂停期”。

公开信表示，暂停应该是从围绕先进技术的“危险竞赛”中后退一步，而不是完全停止通用人工智能（Artificial general
intelligence，AGI）的开发。

一些支持FLI的人认为，研究人员正处于不知不觉中创造出危险的、有知觉的人工智能系统的边缘，就像在《黑客帝国》和《终结者》等流行的科幻电影系列中看到的那样。更加激进的人士如埃利泽·尤德科夫斯基（Eliezer
Yudkowsky）认为，应该无限期暂停新的大规模训练，而且是在全世界范围内暂停。“关闭所有的大型GPU集群（大型计算机农场，最强大的人工智能在这里得到完善）。关闭所有的大型训练运行。对任何人在训练人工智能系统时被允许使用的计算能力设置上限，并在未来几年内将其下调，以补偿更有效的训练算法。立即制定多国协议，防止被禁止的活动转移到其他地方。”

而激烈反对的人甚至失去了礼貌破口大骂。我认识的纽约城市大学新闻学教授杰夫·贾维斯（Jeff
Jarvis）一扫往常心目中的温文印象，在推特上发帖说：“这真XX的可笑，如果我见过的话，这是#道德恐慌#的一个典型标本：呼吁暂停训练人工智能。谷登堡：停止你的印刷机!
有这么多疑为道德企业家和注意力成瘾者的人士签名。”

公平地说，虽然业界绝不会同意暂停六个月，但人工智能发展眼下的发展将速度置于安全之上，的确需要引发社会关注。我认为我们需要敲响尽可能多的警钟，以唤醒监管者、政策制定者和行业领导者。在其他情形下，微妙的警告可能就足够了，但语言模型技术进入市场的速度比此前人们经历过的任何技术都要快。放缓一下脚步，思考一下前路，对社会适应新的人工智能技术是必要的。

但我们看到的情形恰恰相反，在危险的竞赛中，人人都在加码，因为赌注在加大。“一场比赛从今天开始”，微软首席执行官萨蒂亚·纳德拉（Satya Nadella）在
2 月 7 日向谷歌发出了挑战， “我们要行动，而且行动要快”。为应对ChatGPT 的成功，谷歌宣布进入“红色代码”企业紧急状态，并将自己的聊天机器人
Bard 推向市场，在一次演示中表示，它将“重新校准”在发布基于 AI 技术的工具时愿意承担的风险水平。在 Meta
最近的季度财报电话会议上，首席执行官马克·扎克伯格（Mark
Zuckerberg）宣布他的公司目标是“成为生成式人工智能的领导者”。如此热浪滚滚，技术带来的错误和危害必然有所增加——人们对人工智能的反对也会增加。

人工智能安全初创公司SaferAI的首席执行官西米恩·坎珀斯（Simeon
Campos）说，他签署这封信是因为，如果连这些系统的发明者都不知道它们是如何工作的，不清楚它们有什么能力，也不了解对它们的行为如何进行限制，那么就不可能管理系统的风险。
“我们正在将这类系统的能力扩展到前所未有的水平，在全速竞赛中对社会产生变革性影响。必须放慢它们的发展速度，让社会适应并加快替代性的AGI架构，这些架构在设计上是安全的，并且可以正式验证。”

公开信的签署者、纽约大学名誉教授加里·马库斯（Gary
Marcus）相信，这封信将成为一个转折点。“我认为这是AI史上——也许也是人类史上——一个非常重要的时刻。”这个恐怕夸大其词了。公开信在一些重要的地方是错误的。

信中说：“正如大量研究表明的那样，具有人类竞争力的人工智能系统会对社会和人类构成深远风险。”其实风险更多来自于大型语言模型（large language
model, LLM）在一个压迫性体系中的使用，这比假想的人工智能反乌托邦前景要具体和紧迫得多。

不论我们多么惊叹LLMs的“智能”，以及它的学习速度——每天都有各种非常进取的创新出现，微软的一组研究人员在测试了GPT-4之后甚至报告说，它闪现了通用人工智能的火花——然而LLMs在本质上不可能拥有自我意识，它们只是在庞大的文本库中训练出来的神经网络，通过识别模式来产生自己的概率文本。风险和危害从来都非源出强大的人工智能，相反，我们需要担心的是，大型语言模型的权力集中在大玩家手中，复制压迫体系，破坏信息生态系统，以及通过浪费能源资源破坏自然生态系统。

大公司对它们正在做的事情变得越来越神秘，这使得社会很难抵御可能出现的任何危害。公开信没有说到社会正义，而像OpenAI这样的公司依赖于全球南方国家的剥削性劳动，在那里使用时薪不到
2
美元的肯尼亚外包工人来审查有害内容，包括仇恨言论和暴力，以教导人工智能避免这些内容。此外，还有人工智能用于操纵的可能性：现在已经存在通过个性化对话影响活动的技术，这些活动根据用户的价值观、兴趣和背景来定位用户，以推动销售、宣传或错误信息。也就是说，人工智能驱动的对话影响有可能成为人类创造的最强大的有针对性的说服形式。正如马库斯所说，他并不同意其他人对智能机器超出人类控制的担忧，更加担心的是被广泛部署的“平庸的人工智能”，比如被犯罪分子用来欺骗人们或传播危险的错误信息。

**// 编注：**
传统的“南方”“北方”概念出现在上世纪五六十年代，在殖民地解放运动风起云涌的背景下，一些摆脱殖民枷锁走上独立发展道路的国家开始使用“南方”，来表明发展中世界与工业化国家代表的“北方”之间存在系统性不平等。

其他问题包括隐私问题，因为 AI 越来越能够解释大型监控数据集；版权问题，包括 Stability AI
在内的公司正面临来自艺术家和版权所有者的诉讼，他们反对未经许可使用他们的作品来训练 AI 模型；能源消耗问题，每次提示
AI模型时，它都需要能源。人工智能模型使用与游戏计算机相同的强大 GPU 处理器，因为它们需要并行运行多个进程。

公开信渲染人工智能的末日图景，本身也吊诡地充满了人工智能的炒作，使我们更难解决真实的、正在发生的人工智能危害。在这方面，公开信基本上构成了一种误导：把大家的注意力引向假设的LLMs的权力和危害上，并提出一种非常模糊和无效的解决方式，而不是着眼于此时此地的危害并推动解决相关问题——例如，在涉及LLMs的训练数据和能力时要求更多的透明度，或者就它们可以在何处和何时使用进行立法。

公开信提到“人工智能将引起的戏剧性的经济和政治混乱”，实际上，人工智能并不会引发这一切，技术背后的企业和风险投资公司却会。它们希望尽可能地赚取更多的钱，却很少关心技术对民主和环境的影响。危言耸听者大谈特谈人工智能造成的伤害，已经让听者觉得并不是“人”在决定部署这些东西。

碰巧地是，OpenAI也担心人工智能的危险。但是，该公司希望谨慎行事而不是停下来。“我们希望成功应对巨大的风险。在面对这些风险时，我们承认理论上看似正确的事情在实践中往往比预期的更奇怪”，奥特曼在
OpenAI 关于规划通用人工智能的声明中写道。“我们相信我们必须通过部署功能较弱的技术版本来不断学习和适应，以最大限度地减少‘一次成功’的情况。”

换句话说，OpenAI正在正确地遵循人类通常的获取新知识和开发新技术的路径——即从反复试验中学习，而不是通过超自然的先见之明“一次成功”。这种做法是正确的，同时，把人工智能工具交给普通大众使用也是对的，因为“民主化的访问将导致更多更好的研究、分散的权力、更多的利益以及更多的人贡献新想法”。我们只是同时也需要人工智能实验室提供关于训练数据、模型架构和训练制度的透明度，以便全社会更好地研究它们。

微软说它是“由以人为本的道德原则驱动的”；谷歌说它将“大胆而负责任地”向前迈进；而OpenAI说它的使命是用技术“造福全人类”。我们看到许多这样的公司推出了强大的系统，但也在某种程度上推卸责任，并且不太清楚它们将如何管理自身而成为真正的担责者。时间会告诉我们这些宣言是否会有行动的支持，或者充其量只是聊天而已。

**本文独家发布腾讯新闻，未经授权，请勿转载。**

